---
layout: post
title: TF-IDF
meta: "deep learning"
category: ml
author: "Jean-Marc Beaujour"
img_post: 2018-02-10-input_reconstruction.jpg
summary: 
github-link: ""
---

<script src="/js/plotly-latest.min.js"></script>

<script type="text/javascript"
   src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

TF-IDF Term weighting

I na large text corpus, some words will have high frequency providing little information about the actual documents.

If we feed the model directly with direct count data to the classifier, the high frequency data would shadow rarer words.

TF-IDF transforms enable to reweight the count features.

TF = term Frequency
TF-IDF = term frequency time inverse document frequency.

$$
TF-IDF =(t, d) = tf (t, d) * idf(t)
$$

$$idf = log(\frac{1 + n_d}{1 + df(d, t)}) + 1$$