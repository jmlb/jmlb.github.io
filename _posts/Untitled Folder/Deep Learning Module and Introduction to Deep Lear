Deep Learning Module and Introduction to Deep Learning Module (SDC)



# Introduction to Deep learning Module

# Linear Neuron (also called Linear filter)
$$ y = b + \sum x_i w_i = w^T x $$

where $y$ is the output, $w$ is the weight and $x$ the input.

(show plot of y=b+xiwi

1. Binary threshold neurons:

$$z = b +\sum x_i w_i$$

and $y = 1 $ if $z \geq 0$ and $z=0$ otherwise.

Ther are a variety of non-linearity function used: sigmoid, tanh, relu, softmax


2. Rectified Linear Neurons (ReLU)

$z = b + \sum x_i w_i$

$y = z $ if z>0 or 0 otherwise
[show graph]

ReLu helps prevent the vanishing gradient problme because of $y=x$, and add parsity (neurons with sparse activities)

3. Sigmoid neurons
The output of the sigmoid is treated as the probability

$z = b + \sum x_i w_i$

$y = sig(z= \frac{1}{1 + e^{-z}$ and is bound to [0, 1]
[show graph]

4. Stochastic binary neurons
Stochastic binary neurons same as sigmoid neurons. But treat the output $y$

5. softmax
softmax is used for more than 2 classes.
for multiclass classification, we would like to estimate the conditional probability $p(y=c|x)$.

# Minimize Cost
The aim of the learning is to minimize error between target and the actual output.


# Objective function: Cost:
Define the error :

$$ E = \frac{1}{2} \sum (true  - y_i)^2$$ or $$ J9W) = \frac{1}{2m} \sum ({y_hat - y})^2$$

squared error function.

We want to minimize the error i.e :

$$ minimize J(W) $$

$$ w_j : w_j - \alpha \frac{\partial }{\partial \theta_j} J(w)$$


Now differentiate to get error:

$$ \frac{\partial E}{\partial w_i} = \frac{1}{2} \sum (true  - y_i)^2$$

The Delta rule: $\Delta w_i = - \epsilon \frac{\partial E}{\partial w_i}$

Calculate the error surface E


## learning weights of a logistic
Logit: $z = b + \sum x_i w_i$

The output : $y = 1/ (1 + e^{-z})$

Error function: E=(y-y_hat)

Derivatives:

$\frac{\partial z}{\partial w_i} = x_i$ and $\frac{\partial y}{\partial z} = y (1- y)$

$\frac{\partial z}{\partial x_i} = w_i$

$\frac{\partial y}{\partial w_i} =$\frac{\partial z}{\partial w_i} \frac{\partial y}{\partial z} = x_i y (1- y) $

$\frac{\partial E}{\partial w_i} =$ \sum_n \frac{\partial y^n}{\partial w_i} \frac{\partial E}{\partial y^n} = 
 - \sum_n x_i^n y^n (1-y^n) (t^n - y^n) $

$y^n (1-y^n)$ is the slope of logistic unit.



### Neural Network
estimates f(x)_c = p(y=c|x)
We could maximize the probabilities of $y^{t}$ given $x^{t}$ in the training set.

To frame as minimization, we minimize the negative log likelihood:

$$ l(f(x), y) = - \sum_c 1_{y=c} log f(x)c) = - log f(x)_y$$ 

where log is ln

This is sometime refered as cross entropy.


=====================

# Regression and classification

1. For regressiobn : loss = 1/2 9y -t)2 to measure discrepancy

2. for classification

# Neural Network


Cross Entropy D(S, L)

D(y_hat, y) = -sum (y * log y_hat)
			= -tf.reduce_sum(y * log(y_hat))



# regularization L1 or L2: L2 regularizer is also called weight decay
The regularizer penalizes certain values of $\theta$

=====================

Tensorflow
Differentiable graphs are graphs where the nodes are differentiable functions

A neural network is a graph of mathematical functions. The graph consists of neurons (nodes), and links (edges). 


Forward pass : 1st layer to output

There are 2 steps to create a Neural Networks:

1) Define the graph of nodes and edges
2) Propagate value thru the graph
3) Define a loss function l
4) procedure to compute the parameter gradient
5) add regularizer to the gradient
6) initialize method for $\theta$
Initialize b to zero


In Miniflow, you will define the nodes and the edges of your network with one method. Propagate values thru the graph with another method.

inputs [4] ---- [+] (add)
       [10] ---/

Topological sort/Topological ordering


flow graph

[x] ---> a^(1)[x]  -------> h^(1) (x)   -----> a^(2) (x)       --------> f(x)
          w(1), b(1)                           w(2), b(2)

Each box could be an object with a fprop method that computes the value of the box given its parents

Trainign epoch = iteration over all examples.



### transfer learning
Use  pre-trained CNN as an automation feature extraction.

Feature extraction is the process of extracting discreminative information: color histogram or Histogram of oriented gradients, edges, etc..

[image] ---> [cnn feature extractor ] ----> classification[SVM, logistic, etc]