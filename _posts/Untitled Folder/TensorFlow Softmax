TensorFlow Softmax
Now that you've built a softmax function from scratch, let's see how softmax is done in TensorFlow.

x = tf.nn.softmax([2.0, 1.0, 0.2])
Easy as that! tf.nn.softmax() implements the softmax function for you. It takes in logits and returns softmax activations.

Quiz
Use the softmax function in the quiz below to return the softmax of the logits.


simple function with tensorflow

# Solution is available in the other "solution.py" tab
import tensorflow as tf


def run():
    output = None
    logit_data = [2.0, 1.0, 0.1]
    logits = tf.placeholder(tf.float32)
    
    # TODO: Calculate the softmax of the logits
    softmax = tf.nn.softmax(logits)    
    
    with tf.Session() as sess:
        # TODO: Feed in the logit data
        feed_dict = {logits: logit_data}
        output = sess.run(softmax, feed_dict=feed_dict  )
    
    return output





Let's take what you learned from the video and create a cross entropy function in TensorFlow. To create a cross entropy function in TensorFlow, you'll need to use two new functions:

tf.reduce_sum()
tf.log()
Reduce Sum
x = tf.reduce_sum([1, 2, 3, 4, 5])  # 15
The tf.reduce_sum() function takes an array of numbers and sums them together.

Natural Log
x = tf.log(100)  # 4.60517
This function does exactly what you would expect it to do. tf.log() takes the natural log of a number.

Quiz
Print the cross entropy using softmax_data and one_hot_encod_label.

(Alternative link for users in China.)



    # Solution is available in the other "solution.py" tab
import tensorflow as tf

softmax_data = [0.7, 0.2, 0.1]
one_hot_data = [1.0, 0.0, 0.0]

softmax = tf.placeholder(tf.float32)
one_hot = tf.placeholder(tf.float32)


def Xentropy(y, y_):
    return -tf.reduce_sum(tf.mul(y, tf.log(y_)))

with tf.Session() as sess:
    # TODO: Print cross entropy from session
    feed_dict = {softmax: softmax_data, one_hot: one_hot_data}
    output = sess.run(Xentropy(one_hot, softmax), feed_dict=feed_dict  )
    print(output)
