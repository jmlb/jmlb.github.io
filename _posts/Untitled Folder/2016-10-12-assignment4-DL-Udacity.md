---
layout: post
title: Assignment 4 (ConvNet) - Deep Learning (@ Udacity)
comments: true
date: 2016-10-12
tags: ConvNet classifier notMNIST ReLu 
categories: MachineDeepLearning
---


OBJECTIVE: deploy a Convolutional Network to classify **[notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) dataset.**

In the previous assignments (here), we trained fully connected networks to classify 10 letters (A through J). In the assignment here, we will make use of a Convolutional Neural Net for the classification.


### 1. Getting ready: import required modules/libraries and load the data



<script src="https://gist.github.com/jmlb/f25a90d5c3b4f9794a21fa0d52981e25.js"></script>


We now load the pickle file `notMNIST.pickle`. The pickle file contains 3 datasets: training, validation and test set. 


<script src="https://gist.github.com/jmlb/8e846945126fd231e319ea9e99340e0c.js"></script>

The size of the datasets:

    Training set (200000, 28, 28) (200000,)
    Validation set (10000, 28, 28) (10000,)
    Test set (10000, 28, 28) (10000,)



### 2. Reformat data into a TensorFlow-friendly shape

+ convolutions need the image data formatted as a cube (width$$\times$$height$$\times$$NumberOfChannels):

`input(number_of_examples, img_width, img_height, number_of_channels)`


The non-MNIST examples are grayscale images, with a single channel.

+ transform labels with 1-hot encodings.
The 10 classes are converted into sparse vectors of size (10, 1). For example, the label *A* is hot-encoded: `[0, 1, 0, 0, 0, 0, 0, 0]`

<script src="https://gist.github.com/jmlb/bed7a24ec5f9285fb67e593e90bebf24.js"></script>


<script src="https://gist.github.com/jmlb/2652c5038c6332a409fa8942eedd9ee1.js"></script>


##### DROP OUT
This scaling enables the same network to be used for training (with keep_prob < 1.0) and evaluation (with keep_prob == 1.0). From the Dropout paper:

The idea is to use a single neural net at test time without dropout. The weights of this network are scaled-down versions of the trained weights. If a unit is retained with probability p during training, the outgoing weights of that unit are multiplied by p at test time as shown in Figure 2.
Rather than adding ops to scale down the weights by keep_prob at test time, the TensorFlow implementation adds an op to scale up the weights by 1. / keep_prob at training time. The effect on performance is negligible, and the code is simpler (because we use the same graph and treat keep_prob as a tf.placeholder() that is fed a different value depending on whether we are training or evaluating the network).
The aim is to keep the expected sum of the weights the same&mdash;and hence the expected value of the activations the same&mdash;regardless of keep_prob. If (when doing dropout) we disable a neuron with probability keep_prob, we need to multiply the other weights by 1. / keep_prob to keep this value the same (in expectation). Otherwise, for example, the non-linearity would produce a completely different result depending on the value of keep_prob. â€“ mrry Jan 4 at 19:29



### 3. Build a small ConvNet

Our first Convolutional Network has a simple architecture: two convolutional layers are followed by one fully connected layer. Because Convolutional Networks are computationally expensive, we limit the depth of the Conv layer as well as the number of fully connected nodes.
The first conv layer is generated by sliding above the image 16 filters with window size (5, 5). The stride of (2, 2) and the `padding = NORMAL`, i.e our Conv layer will have the same size as our input layer.
For the 2nd Conv layer, there are also 16 filters, and stride (2, 2) and padding=`NORMAL`. The hidden layer of the fully connected layer has 90 nodes.

![png](GRAPH)

<script src="https://gist.github.com/jmlb/4460c6b7625c9c3074cb079c8c948564.js"></script>

The accuracy of this simple convNet on the validation set and on the test set is respectively: .... and 89.3%. Note that we trained the convNet for 1000 step. The accuracy is actually is lower than what we get when we used a fully connected Neural Net architecture (ref. assignment 3). But we might be able to improve by changing the arcitecture of the ConvNet and the hyper-parameters.


### 4. Problem 1: maxpool

In the convolutional model above, we use convolutions with stride 2 to reduce the dimensionality. In the following model, we replace the strides by a **max pooling** operation (`nn.max_pool()`) of stride 2 and kernel size 2.


<script src="https://gist.github.com/jmlb/ba47d7c8c9121fe0f7f5f8c2737a3f4b.js"></script>

 
When replacing the convolution by the pooling, our accuracy on the validation and on the test set drops significantly : **Test accuracy: 80.7%**.
This coudl be because the **max pooling** is loosing too much information in the subsampling process: the **max pooling** retains only the pixels with the highest value in the window.

![png](output_12_1.png)


### Problem 2: Improve the model performance

The table below summarizes how the parameter and the model was modified in order to improve the model accuracy. 
In order to perform the model performance:Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.


<script src="https://gist.github.com/jmlb/3f6219091d444e81cc53df22c6d442b2.js"></script>


![png](output_16_1.png)


<script type="text/javascript">

        function Div(id,ud) {
           var div=document.getElementById(id);
           var h=parseInt(div.style.height)+ud;
           if (h>=1){
              div.style.height = h + "em"; // I'm using "em" instead of "px", but you can use px like measure...
           }
        }
      </script>



### LeNet 5 Architecture

+ input is 28x28 (1 channel i.e grey scale)
+ layer C1 : conv 6 feature map 5*5
+ layer S1 : mappool 2*2 6 feature maps
+ layer C3 : conv 16 featyure maps 6*6
+ layer S4 : conv 16 feature maps 2*2 kernels
+ layer C5 : conv 120 feature maps 6*6 kernels
+ layer C6 : fully connected - 1 hidden layer with 84 nodes

add bias 
Non linearity ReLu, tanh sigmoid or softmax.
